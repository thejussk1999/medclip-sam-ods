{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image,ImageDraw\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import SamModel\n",
    "from torch.optim import Adam\n",
    "import monai\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "from torch.nn.functional import threshold, normalize\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from scipy.ndimage import label, binary_fill_holes\n",
    "from torchvision.ops import nms\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor, resize\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_points(array_size=1024, grid_size=5):\n",
    "    x = np.linspace(0, array_size-1, grid_size)\n",
    "    y = np.linspace(0, array_size-1, grid_size)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    input_points = [[[int(x), int(y)] for x, y in zip(x_row, y_row)] for x_row, y_row in zip(xv.tolist(), yv.tolist())]\n",
    "    input_points = torch.tensor(input_points).view(1, 1, grid_size*grid_size, 2)  # Reshape as needed\n",
    "    return input_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_masks(masks, kernel_size=3):\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))\n",
    "    refined_masks = np.zeros_like(masks)\n",
    "    for i in range(masks.shape[0]):\n",
    "        mask = masks[i]\n",
    "        # Fill holes\n",
    "        mask_filled = binary_fill_holes(mask).astype(np.uint8)\n",
    "        # Open operation to remove noise\n",
    "        mask_refined = cv2.morphologyEx(mask_filled, cv2.MORPH_OPEN, kernel)\n",
    "        refined_masks[i] = mask_refined\n",
    "    return refined_masks\n",
    "\n",
    "\n",
    "def extract_bounding_boxes(masks, iou_threshold=0.1, min_area=1, max_area=1000, aspect_ratio_range=(0.5, 2)):\n",
    "\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    for i in range(masks.shape[0]):\n",
    "        mask = masks[i].astype(np.uint8) * 255\n",
    "\n",
    "        #Ensure the mask is 2D\n",
    "        if mask.ndim > 2:\n",
    "            mask = mask.squeeze()\n",
    "        \n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if area < min_area or area > max_area:\n",
    "                continue\n",
    "            \n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            aspect_ratio = w / float(h)\n",
    "            if not (aspect_ratio_range[0] <= aspect_ratio <= aspect_ratio_range[1]):\n",
    "                continue\n",
    "            \n",
    "            score = area  \n",
    "        \n",
    "            boxes.append([x, y, x+w, y+h])\n",
    "            scores.append(score)\n",
    "\n",
    "    if boxes:\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        scores = torch.as_tensor(scores, dtype=torch.float32)\n",
    "        keep = nms(boxes, scores, iou_threshold)\n",
    "        return boxes[keep].numpy(), scores[keep].numpy()\n",
    "    else:\n",
    "        return np.array([]), np.array([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "image_directory = '/Users/sahiljethani/Desktop/MLP/archive/preprocessed_train'\n",
    "\n",
    "df_test = pd.read_csv('/Users/sahiljethani/Desktop/MLP/flattened_df_normalized.csv')\n",
    "\n",
    "# Load the model configuration\n",
    "model_config = SamConfig.from_pretrained(\"facebook/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    " # Create an instance of the model architecture with the loaded configuration\n",
    "model = SamModel(config=model_config)\n",
    "model.load_state_dict(torch.load(\"/Users/sahiljethani/Desktop/MLP/epochs_4_model_checkpoint.pth\", map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.nunique()\n",
    "df_test['list_pred_bboxes_sam_grid_4epoch']=None\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, image_id, output_path):\n",
    "    image = Image.open(image_path) # Load grayscale image\n",
    "    image = to_tensor(image)  # Convert to PyTorch tensor\n",
    "    image = image.repeat(3, 1, 1)  # Repeat the single channel to get 3 channels\n",
    "    image = to_pil_image(image)  # Convert back to PIL Image\\    \n",
    "    image_save_path = os.path.join(output_path, image_id + '.png')\n",
    "    image.save(image_save_path) \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mask_prob=0.6\n",
    "iou_threshold=0.1\n",
    "min_area=0.6 \n",
    "\n",
    "test_image_directory='/Users/sahiljethani/Desktop/MLP/archive/preprocessed_test'\n",
    "\n",
    "\n",
    "def makedataset(df,A,B):\n",
    "    for image_id in df[A:B]['image_id']:\n",
    "        image_path = os.path.join(test_image_directory, f\"{image_id}.png\")\n",
    "        # image = Image.open(image_path)\n",
    "        image=load_and_preprocess_image(image_path)\n",
    "        input_points = generate_input_points(grid_size=5)\n",
    "\n",
    "        inputs = processor(images=image, input_points=input_points, return_tensors=\"pt\")\n",
    "\n",
    "        # Move inputs to the correct device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, multimask_output=False)\n",
    "\n",
    "      \n",
    "        # Post-process the output\n",
    "        pred_masks_prob_new =torch.sigmoid(outputs.pred_masks.squeeze(1)).cpu().numpy().squeeze(1)\n",
    "        pred_masks_new = (pred_masks_prob_new > mask_prob).astype(np.uint8)\n",
    "\n",
    "        refined_masks = refine_masks(pred_masks_new.astype(np.uint8))\n",
    "        boxes, _ = extract_bounding_boxes(refined_masks,iou_threshold=iou_threshold, min_area=min_area, max_area=50000, aspect_ratio_range=(0.5, 2))\n",
    "\n",
    "        for box in boxes:\n",
    "            #divide each bbox by 256\n",
    "            box[0]=box[0]/256\n",
    "            box[1]=box[1]/256\n",
    "            box[2]=box[2]/256\n",
    "            box[3]=box[3]/256\n",
    "        boxes = boxes.tolist()\n",
    "        boxes=str(boxes)\n",
    "\n",
    "        #storing the list of boxes in the dataframe\n",
    "        df_test.loc[df_test['image_id'] == image_id, 'list_pred_bboxes_sam_grid_4epoch'] = boxes\n",
    "    return df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=0\n",
    "B=3000\n",
    "df_new=makedataset(df_test,A,B)\n",
    "save_path = '/Users/sahiljethani/Desktop/MLP/testcsv/bbox_eval_sam_{}_{}.csv'.format(A,B)\n",
    "df_new.to_csv(save_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
